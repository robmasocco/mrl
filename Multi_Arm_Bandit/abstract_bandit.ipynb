{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules.\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit(ABC):\n",
    "\n",
    "    def __init__(self, means, sigmas, arms=10, iters=10000, deterministic=False, stationary=True):\n",
    "        \"\"\"Initializes the abstarct bandit object and model attributes.\"\"\"\n",
    "        # Model variables initialization.\n",
    "        # Means of the normal distribution for rewards.\n",
    "        self.means = means\n",
    "        # Standard deviations of the normal distribution for rewards.\n",
    "        self.sigmas = sigmas\n",
    "        # Number of arms.\n",
    "        self.arms = arms\n",
    "        # Counters for actions taken.\n",
    "        self.Ns = np.zeros(arms)\n",
    "        # Number of iterations of the episode.\n",
    "        self.iters = iters\n",
    "        # If True, rewards are deterministic.\n",
    "        self.deterministic = deterministic\n",
    "        # If True, means don't change.\n",
    "        self.stationary = stationary\n",
    "\n",
    "        # Simulation data initialization.\n",
    "        # Array to store the trend of the avarage reward.\n",
    "        self.avg_rewards = np.zeros(iters + 1)\n",
    "        # Matrix to store the trend of the means.\n",
    "        self.qs = np.zeros((iters + 1, arms))\n",
    "        self.qs[0, :] = self.means\n",
    "        # Counter for optimal actions taken.\n",
    "        self.opt_actions = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def choose_action(self):\n",
    "        pass\n",
    "        \n",
    "    def pull_arm(self, arm):\n",
    "        \"\"\"Draws a reward using a normal distribution you can set.\"\"\"\n",
    "        if self.deterministic == True:\n",
    "            return self.means[arm]\n",
    "        else:\n",
    "            return np.random.normal(self.means[arm], self.sigmas[arm])\n",
    "        \n",
    "    @abstractmethod\n",
    "    def update_model(self, arm, reward, iteration):\n",
    "        pass        \n",
    "\n",
    "    @abstractmethod\n",
    "    def update_sim_data(self, arm , reward, iteration):\n",
    "        pass\n",
    "            \n",
    "    def run(self):\n",
    "        \"\"\"Runs a set of episodes.\"\"\"\n",
    "        for i in range(self.iters):\n",
    "            arm = self.choose_action()\n",
    "            reward = self.pull_arm(arm)\n",
    "            self.update_model(arm, reward, i)\n",
    "            self.update_sim_data(arm, reward, i)\n",
    "                \n",
    "    def get_avg_rewards(self):\n",
    "        \"\"\"Return the trend of the avarage reward.\"\"\"\n",
    "        return np.copy(self.avg_rewards)\n",
    "    \n",
    "    def get_real_means(self):\n",
    "        \"\"\"Returns real actions values.\"\"\"\n",
    "        return np.copy(self.qs)\n",
    "\n",
    "    def get_actions(self):\n",
    "        \"\"\"Returns actions counters.\"\"\"\n",
    "        return np.copy(self.Ns)\n",
    "\n",
    "    def get_opt_actions(self):\n",
    "        \"\"\"Return percentage of optimal actions taken.\"\"\"\n",
    "        return float(self.opt_actions) / float(self.iters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
