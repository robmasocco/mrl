{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference Update Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "import import_ipynb\n",
    "from abstract_bandit import Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefBandit(Bandit):\n",
    "\n",
    "    def __init__(self, means, sigmas, arms=10, iters=10000, alpha=None, beta=None, deterministic=False, stationary=True):\n",
    "        \"\"\"Initializes the Upper Confidence Bound bandit.\"\"\"\n",
    "        super().__init__(means, sigmas, arms=arms, iters=iters, deterministic=deterministic, stationary=stationary)\n",
    "        # Step size for preferences.\n",
    "        self.alpha = alpha\n",
    "        # Step size for average reward.\n",
    "        self.beta = beta\n",
    "        # Preference values of actions.\n",
    "        self.prefs = np.zeros(arms)\n",
    "        # Probability of choosing action.\n",
    "        self.probs = np.full(arms, 1.0 / float(arms))\n",
    "        # Matrix to store the trend of preferences.\n",
    "        self.Hs = np.zeros((iters + 1, arms))\n",
    "        # Matrix to store the trend of probabilities.\n",
    "        self.Ps = np.zeros((iters + 1, arms))\n",
    "        self.Ps[0, :] = self.probs\n",
    "\n",
    "    def choose_action(self):\n",
    "        \"\"\"Implements the epsilon-greedy policy: exploits knowledge or explores new possibilities.\"\"\"\n",
    "        return np.random.choice(list(range(self.arms)), p=self.probs)\n",
    "\n",
    "    def update_model(self, arm, reward, iteration):\n",
    "        \"\"\"Updates estimates and other data after an episode.\"\"\"\n",
    "        # Update choice count.\n",
    "        self.Ns[arm] += 1\n",
    "        # Update step size for preferences.\n",
    "        if self.alpha == None:\n",
    "            step_size_pref = 1.0 / float(self.Ns[arm])\n",
    "        else:\n",
    "            step_size_pref = self.alpha\n",
    "        # Update step size for average rewards.\n",
    "        if self.beta == None:\n",
    "            step_size_rew = 1.0 / float(iteration + 1)\n",
    "        else:\n",
    "            step_size_rew = self.beta\n",
    "        # Update average reward (note that now this has to be done here!).\n",
    "        self.avg_rewards[iteration + 1] = self.avg_rewards[iteration] + step_size_rew * (reward - self.avg_rewards[iteration])\n",
    "        # Update preferences.\n",
    "        for i in range(arms):\n",
    "            if i == arm:\n",
    "                self.prefs[i] += step_size_pref * (reward - self.avg_rewards[iteration + 1]) * (1.0 - self.probs[i])\n",
    "            else:\n",
    "                self.prefs[i] -= step_size_pref * (reward - self.avg_rewards[iteration + 1]) * self.probs[i]\n",
    "        # Update actions probabilities.\n",
    "        exp_sum = np.sum(np.exp(self.prefs))\n",
    "        self.probs = np.exp(self.prefs) / exp_sum\n",
    "        # Eventually increase optimal actions counter.\n",
    "        if arm == np.argmax(self.means):\n",
    "            self.opt_actions += 1.0\n",
    "        # Update real action values.\n",
    "        if self.stationary == False:\n",
    "            for i in range(self.arms):\n",
    "                self.means[i] += self.means_rng.standard_normal()\n",
    "\n",
    "    def update_sim_data(self, arm , reward, iteration):\n",
    "        \"\"\"Updates simulation data after an episode.\"\"\"\n",
    "        self.Hs[iteration + 1, :] = self.prefs\n",
    "        self.Ps[iteration + 1, :] = self.probs\n",
    "        self.qs[iteration + 1, :] = self.means\n",
    "\n",
    "    def get_prefs(self):\n",
    "        \"\"\"Returns actions preferences.\"\"\"\n",
    "        return np.copy(self.Hs)\n",
    "    \n",
    "    def get_probs(self):\n",
    "        \"\"\"Returns actions probabilities.\"\"\"\n",
    "        return np.copy(self.Ps)\n",
    "\n",
    "def pref_plots(iters, arms, means, sigmas, alphas, betas, det, stat):\n",
    "    \"\"\"Function that generates test plots for this Bandit.\"\"\"\n",
    "    # Simulation data arrays.\n",
    "    rews = np.zeros((len(alphas), len(betas), iters + 1))\n",
    "    actions = np.zeros((len(alphas), len(betas), arms))\n",
    "    optimals = np.zeros((len(alphas), len(betas)))\n",
    "    Hs_list = []\n",
    "    Ps_list = []\n",
    "\n",
    "    # Bandits execution.\n",
    "    for i in range(len(alphas)):\n",
    "        Hs_temp = []\n",
    "        Ps_temp = []\n",
    "        for j in range(len(betas)):\n",
    "            bandit = PrefBandit(np.copy(means), sigmas, arms=arms, iters=iters, alpha=alphas[i], beta=betas[j], deterministic=det, stationary=stat)\n",
    "            bandit.run()\n",
    "            rews[i, j, :] = bandit.get_avg_rewards()\n",
    "            actions[i, j, :] = bandit.get_actions()\n",
    "            optimals[i, j] = bandit.get_opt_actions()\n",
    "            Hs_temp.append(bandit.get_prefs())\n",
    "            Ps_temp.append(bandit.get_probs())\n",
    "        Hs_list.append(Hs_temp)\n",
    "        Ps_list.append(Ps_temp)\n",
    "    \n",
    "    colors = cm.brg(np.linspace(0, 1, len(betas)))\n",
    "    for i in range(len(alphas)):\n",
    "        fig = plt.figure(figsize=(20,40))\n",
    "        plt.subplots_adjust(top=0.92)\n",
    "        fig.suptitle(r'$\\alpha$ = ' + str(alphas[i]))\n",
    "        gs = gridspec.GridSpec(1+arms, 2, figure=fig)\n",
    "        # Average rewards plot.\n",
    "        ax1 = plt.subplot(gs[0, 0])\n",
    "        for j in range(len(betas)):\n",
    "            ax1.plot(rews[i, j, :], color=colors[j], label=r'$\\beta$ = ' + str(betas[j]))\n",
    "        ax1.legend(loc='upper right')\n",
    "        ax1.set(xlabel='Iterations', ylabel='Avg. rewards')\n",
    "        ax1.set_title('Average rewards')\n",
    "        # Actions taken plot\n",
    "        ax2 = plt.subplot(gs[0, 1])\n",
    "        x = np.arange(arms)\n",
    "        width = 1\n",
    "        pos = list(range(1 - len(betas), len(betas), 2))\n",
    "        for j in range(len(betas)):\n",
    "            ax2.bar(x * width + pos[j] / (2.0 * len(betas)), actions[i, j, :], width / len(betas), color=colors[j], label=r'$\\beta$ = ' + str(betas[j]))\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(np.arange(1, arms+1))\n",
    "        ax2.set(xlabel='Actions', ylabel='Number of actions taken')\n",
    "        ax2.set_title('Actions taken')\n",
    "        # Real mean vs estimated means subplots.\n",
    "        for k in range(arms):\n",
    "            # Preferences plots.\n",
    "            ax = plt.subplot(gs[k+1, 0])\n",
    "            ax.set_title(\"Pref Arm \" + str(k+1))\n",
    "            for j in range(len(betas)):\n",
    "                Hs_current = Hs_list[i][j]\n",
    "                ax.plot(Hs_current[:, k], color=colors[j], label=r'$\\beta$ = ' + str(betas[j]))\n",
    "            ax.legend(loc='upper right')\n",
    "            # Probabilities plots.\n",
    "            ax = plt.subplot(gs[k+1, 1])\n",
    "            ax.set_title(\"Prob Arm \" + str(k+1))\n",
    "            for j in range(len(betas)):\n",
    "                Ps_current = Ps_list[i][j]\n",
    "                ax.plot(Ps_current[:, k], color=colors[j], label=r'$\\beta$ = ' + str(betas[j]))\n",
    "            ax.legend(loc='upper right')\n",
    "            \n",
    "    # Optimal actions frequency heatmap.\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    im = ax.imshow(optimals, cmap='Blues')\n",
    "    ax.set_xticks(np.arange(len(betas)))\n",
    "    ax.set_yticks(np.arange(len(alphas)))\n",
    "    ax.set_xticklabels(map(str, betas))\n",
    "    ax.set_yticklabels(map(str, alphas))\n",
    "    ax.set_title(\"Percentages of optimal actions taken\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    for i in range(len(alphas)):\n",
    "        for j in range(len(betas)):\n",
    "            text = ax.text(j, i, optimals[i, j], ha=\"center\", va=\"center\", color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic, stationary case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters (some MUST be floats!).\n",
    "iters = 5000\n",
    "arms = 10\n",
    "means = np.arange(1.0, float(arms + 1), 1.0)\n",
    "sigmas = None\n",
    "alphas = [None, 0.0025, 0.005, 0.0075, 0.01]\n",
    "betas = [None, 0.025, 0.05, 0.075, 0.1]\n",
    "det = True\n",
    "stat = True\n",
    "\n",
    "pref_plots(iters, arms, means, sigmas, alphas, betas, det, stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic, non-stationary case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters (some MUST be floats!).\n",
    "iters = 5000\n",
    "arms = 10\n",
    "means = np.full(arms, 0.0)\n",
    "sigmas = None\n",
    "alphas = [None, 0.0025, 0.005, 0.0075, 0.01]\n",
    "betas = [None, 0.025, 0.05, 0.075, 0.1]\n",
    "det = True\n",
    "stat = False\n",
    "\n",
    "pref_plots(iters, arms, means, sigmas, alphas, betas, det, stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic, stationary case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters (some MUST be floats!).\n",
    "iters = 5000\n",
    "arms = 10\n",
    "means = np.arange(1.0, float(arms + 1), 1.0)\n",
    "sigmas = np.full(arms, 1.0)\n",
    "alphas = [None, 0.0025, 0.005, 0.0075, 0.01]\n",
    "betas = [None, 0.025, 0.05, 0.075, 0.1]\n",
    "det = False\n",
    "stat = True\n",
    "\n",
    "pref_plots(iters, arms, means, sigmas, alphas, betas, det, stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic, non-stationary case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters (some MUST be floats!).\n",
    "iters = 5000\n",
    "arms = 10\n",
    "means = np.full(arms, 0.0)\n",
    "sigmas = np.full(arms, 1.0)\n",
    "alphas = [None, 0.0025, 0.005, 0.0075, 0.01]\n",
    "betas = [None, 0.025, 0.05, 0.075, 0.1]\n",
    "det = False\n",
    "stat = False\n",
    "\n",
    "pref_plots(iters, arms, means, sigmas, alphas, betas, det, stat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
