{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd01af28833a6dc7d1a151ae007b2488fb1dd1aef320dbb62d15c3e383364d1e2c5",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Monte Carlo Race Car\n",
    "**Authors: Alessandro Tenaglia, Roberto Masocco, Giacomo Solfizi**\n",
    "\n",
    "**Date: May 24, 2021**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.7.9)\nHello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "%run RaceCar.py\n",
    "import numpy as np\n",
    "import pygame"
   ]
  },
  {
   "source": [
    "## Environment Creation\n",
    "Some notes:\n",
    "- The map is randomly generated inside a window of fixed size (and actually quite large).\n",
    "- A reward of 0 is given when the finish line is reached.\n",
    "- A reward of around -900 is given when an obstacle is hit.\n",
    "- A reward of -1 is given for each regular step made."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_map(w_cells, h_cells, min_lim, max_lim):\n",
    "    \"\"\"Generates a map as a table in which each cell holds a numerical flag.\"\"\"\n",
    "    race_map = np.zeros((w_cells, h_cells), dtype=np.int32)\n",
    "    # Set boundaries and obstacles: -1.\n",
    "    race_map[w_cells // 2:, h_cells // 2:] = -1\n",
    "    lims = np.random.randint(min_lim, max_lim, size=4)\n",
    "    for h in range(h_cells):\n",
    "        lims[0] = np.amax([min_lim, np.amin([max_lim, lims[0] + np.random.choice([-1, 0, 1])])])\n",
    "        race_map[h, :lims[0]] = -1\n",
    "        if h > h_cells // 2:\n",
    "            lims[1] = np.amax([min_lim, np.amin([max_lim, lims[1] + np.random.choice([-1, 0, 1])])])\n",
    "            race_map[h, h_cells // 2 - lims[1]:] = -1\n",
    "    for w in range(w_cells):\n",
    "        lims[2] = np.amax([min_lim, np.amin([max_lim, lims[2] + np.random.choice([-1, 0, 1])])])\n",
    "        race_map[:lims[2], w] = -1\n",
    "        if w > w_cells // 2:\n",
    "            lims[3] = np.amax([min_lim, np.amin([max_lim, lims[3] + np.random.choice([-1, 0, 1])])])\n",
    "            race_map[w_cells // 2 - lims[3]:, w] = -1\n",
    "    # Set start line: 1.\n",
    "    race_map[-1, np.argwhere(race_map[-1, :] == 0)] = 1\n",
    "    # Set finish line: 2.\n",
    "    race_map[np.argwhere(race_map[:, -1] == 0), -1] = 2\n",
    "    return race_map\n",
    "\n",
    "# Initialize map dimensions and generate it.\n",
    "width = 850\n",
    "height = 850\n",
    "w_cells = 30\n",
    "h_cells = 30\n",
    "min_lim = 1\n",
    "max_lim = 6\n",
    "race_map = generate_map(w_cells, h_cells, min_lim, max_lim)\n",
    "race_map.dump(\"RaceMap.dat\")"
   ]
  },
  {
   "source": [
    "## Monte Carlo algorithm execution\n",
    "Some notes:\n",
    "- Actions are (X, Y) movements, namely \"horizontal\" and \"vertical\", inside the map, named \"velocities\".\n",
    "- Along Y, the car can move from 0 up to 5 tiles upwards.\n",
    "- Along X, the car can move from 0 to 5 tiles to the right, or 1 to the left (\"reverse gear\").\n",
    "- The null action, i.e. (0, 0) is allowed, but the optimal estimated policy should never take it.\n",
    "- Maps are usually large, so no less than 500000 episodes should be run. In any case, if the finish line is never reached the results are meaningless since the policy estimated will be suicidal."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting line has 8 state(s).\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "Visited 13533 (state, action) pairs.\n",
      "Reached the finish line 595608 time(s).\n"
     ]
    }
   ],
   "source": [
    "# Initialize states, the identify initial and terminal states.\n",
    "S = race_map.shape[0] * race_map.shape[1]\n",
    "init_states = np.argwhere(race_map == 1)\n",
    "terminal_states = np.argwhere(race_map == 2)\n",
    "wins = 0\n",
    "print(\"Starting line has {} state(s).\".format(len(init_states)))\n",
    "# Number of actions.\n",
    "actions_x = 7\n",
    "actions_y = 6\n",
    "A = actions_x * actions_y\n",
    "# Randomly initialize the policy.\n",
    "pi = np.random.randint(A, size=S)\n",
    "# Discount factor.\n",
    "gamma = 1.0\n",
    "# Exploration degree.\n",
    "epsilon = 0.1\n",
    "# Number of episodes.\n",
    "episodes = 1000000\n",
    "# Inizialize MC data: N counters and q estimate (randomly).\n",
    "N = np.zeros((S, A), dtype=np.int32)\n",
    "Q = np.random.normal(loc=10.0, size=(S, A))\n",
    "for T in terminal_states:\n",
    "    # Terminal states have null value.\n",
    "    Q[np.ravel_multi_index((T[0], T[1]), (race_map.shape[0], race_map.shape[1])), :] = 0.0\n",
    "\n",
    "# Loop on episodes.\n",
    "for e in range(episodes):\n",
    "    if (e % 10000) == 0:\n",
    "        # Tell us where you at.\n",
    "        print(e)\n",
    "    # Exploring start.\n",
    "    init_coords = np.copy(init_states[np.random.randint(init_states.shape[0])])\n",
    "    # Run a game.\n",
    "    car = RaceCar(race_map, init_coords, pi, epsilon, actions_x, actions_y)\n",
    "    if car.run() == True:\n",
    "        wins += 1\n",
    "    # Perform MC update.\n",
    "    states = car.get_states()\n",
    "    actions = car.get_actions()\n",
    "    rewards = car.get_rewards()\n",
    "    G = 0.0\n",
    "    for t, s_t in reversed(list(enumerate(states))):\n",
    "        G = rewards[t] + gamma * G\n",
    "        N[s_t, actions[t]] += 1\n",
    "        Q[s_t, actions[t]] += (1.0 / N[s_t, actions[t]]) * (G - Q[s_t, actions[t]])\n",
    "        pi[s_t] = np.argmax(Q[s_t, :])\n",
    "if wins == 0:\n",
    "    print(\"ERROR: Never got to the finish line!\")\n",
    "    raise\n",
    "print(\"Visited {} (state, action) pairs.\".format(np.count_nonzero(N)))\n",
    "print(\"Reached the finish line {} time(s).\".format(wins))\n",
    "\n",
    "# Save results.\n",
    "N.dump(\"N.dat\")\n",
    "Q.dump(\"Q.dat\")\n",
    "pi.dump(\"pi.dat\")"
   ]
  }
 ]
}